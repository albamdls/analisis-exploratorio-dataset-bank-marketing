{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca6c8fdd",
   "metadata": {},
   "source": [
    "# üìä An√°lisis exploratorio: üè¶ Bank Marketing Dataset (UCI)\n",
    "\n",
    "## 1Ô∏è‚É£ **Introducci√≥n y contexto del proyecto**\n",
    "\n",
    "El an√°lisis exploratorio que se va a realizar est√° basado en el dataset **Bank Marketing**, publicado por el Banco de Portugal y disponible en el repositorio UCI Machine Learning:  \n",
    "üîó https://archive.ics.uci.edu/dataset/222/bank+marketing  \n",
    "\n",
    "Este conjunto de datos recoge informaci√≥n de campa√±as de marketing telef√≥nico realizadas por una entidad bancaria portuguesa. El banco contactaba a clientes potenciales con el fin de ofrecerles un dep√≥sito a plazo fijo. Cada fila representa a un cliente contactado y las variables recogen aspectos demogr√°ficos, financieros y operativos del proceso de captaci√≥n. Los datos recogidos en este dataset est√°n ordenadores por fecha, desde Mayo del 2008 a Noviembre del 2010.\n",
    "\n",
    "El fichero que analizaremos, **bank-full.csv**, contiene **45.211 registros** y **17 variables** relacionadas con datos demogr√°ficos, laborales, financieros y caracter√≠sticas de la interacci√≥n comercial.\n",
    "\n",
    "Aunque el objetivo original del dataset era predecir si un cliente contratar√≠a un dep√≥sito a plazo fijo, en este proyecto vamos a reutilizar estas variables para analizar y modelar dos grandes escenarios reales dentro del sector financiero: **gesti√≥n de patrimonio** y **gesti√≥n del riesgo**.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **Objetivo del an√°lisis**\n",
    "\n",
    "El prop√≥sito de este an√°lisis es **comprender el perfil financiero y sociodemogr√°fico de los clientes del banco**, analizando sus patrones econ√≥micos y su posible estabilidad o capacidad de ahorro.\n",
    "\n",
    "Para ello, exploraremos en profundidad la columna `balance`, que refleja el **saldo anual promedio de la cuenta bancaria del cliente**. A partir de esta m√©trica construiremos **dos posibles variables objetivo (targets)**, inspiradas en escenarios reales del sector bancario:\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## üÖ∞Ô∏è **Escenario A: El Umbral del 90% ‚Äî Gesti√≥n de Patrimonio**\n",
    "\n",
    "En banca privada existe un fen√≥meno conocido: **el 10% de los clientes m√°s ricos generan alrededor del 80% de los ingresos del banco** (Principio de Pareto). Identificar este grupo es clave para priorizar servicios personalizados y productos de alto valor.\n",
    "\n",
    "**Acci√≥n realista:**  \n",
    "Calcular el **percentil 90 (P90)** del `balance`. Ese valor representa el **m√≠nimo saldo que posee el 10% de clientes m√°s adinerados**.\n",
    "\n",
    "A partir de ese umbral crearemos la variable binaria `Patrimonio_Alto`:\n",
    "\n",
    "- `Patrimonio_Alto = 1` si `balance > P90`  \n",
    "- `Patrimonio_Alto = 0` si `balance ‚â§ P90`\n",
    "\n",
    "**Interpretaci√≥n pr√°ctica:**  \n",
    "El modelo resultante permitir√° estimar si un cliente **pertenece al segmento de alto patrimonio**, a partir de su edad, trabajo, educaci√≥n, historial crediticio y comportamiento financiero.  \n",
    "Esto imita c√≥mo las divisiones de **Wealth Management** detectan clientes con potencial para productos premium.\n",
    "\n",
    "---\n",
    "\n",
    "## üÖ±Ô∏è **Escenario B: El Umbral Cero ‚Äî Riesgo y Sobregiro**\n",
    "\n",
    "En gesti√≥n de riesgos bancarios, uno de los indicadores m√°s cr√≠ticos es detectar **clientes con saldo negativo** o riesgo de entrar en sobregiro. Un balance por debajo de cero se√±ala inestabilidad financiera o dependencia del cr√©dito.\n",
    "\n",
    "**Acci√≥n realista:**  \n",
    "Utilizar el valor **0 como umbral**.\n",
    "\n",
    "Crearemos la variable binaria `Riesgo_Sobregiro`:\n",
    "\n",
    "- `Riesgo_Sobregiro = 1` si `balance < 0`  \n",
    "- `Riesgo_Sobregiro = 0` si `balance ‚â• 0`\n",
    "\n",
    "**Interpretaci√≥n pr√°ctica:**  \n",
    "El modelo predecir√° qu√© clientes tienen una **alta probabilidad de estar en n√∫meros rojos**, lo que resulta esencial para los departamentos de **Risk Management**.  \n",
    "Esto ayuda a anticipar comportamientos financieros que puedan derivar en impagos o necesidad de intervenci√≥n.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© **Significado de las variables del dataset**\n",
    "\n",
    "| **Columna** | **Descripci√≥n** |\n",
    "|-------------|-----------------|\n",
    "| `age` | Edad del cliente. |\n",
    "| `job` | Tipo de trabajo u ocupaci√≥n. |\n",
    "| `marital` | Estado civil. |\n",
    "| `education` | Nivel de educaci√≥n alcanzado. |\n",
    "| `default` | ¬øTiene impagos en cr√©ditos anteriores? |\n",
    "| `balance` | Saldo medio anual de la cuenta bancaria. |\n",
    "| `housing` | ¬øTiene hipoteca en curso? (yes/no) |\n",
    "| `loan` | ¬øTiene alg√∫n pr√©stamo personal? |\n",
    "| `contact` | Tipo de contacto empleado (tel√©fono m√≥vil o fijo). |\n",
    "| `day` | D√≠a del mes en que se realiz√≥ el √∫ltimo contacto. |\n",
    "| `month` | Mes del √∫ltimo contacto. |\n",
    "| `duration` | Duraci√≥n (en segundos) de la √∫ltima llamada. |\n",
    "| `campaign` | N√∫mero de contactos realizados durante la campa√±a. |\n",
    "| `pdays` | D√≠as desde el √∫ltimo contacto previo a esta campa√±a. |\n",
    "| `previous` | N√∫mero de contactos en campa√±as anteriores. |\n",
    "| `poutcome` | Resultado de campa√±as anteriores. |\n",
    "| `y` | Variable original del banco: ¬øsuscribi√≥ el dep√≥sito a plazo? (yes/no). |\n",
    "\n",
    "---\n",
    "\n",
    "üìå *Fuente de los datos: UCI Machine Learning Repository, Bank Marketing Dataset (2012).*  \n",
    "\n",
    "Este an√°lisis nos permitir√° comprender mejor los perfiles financieros de los clientes y construir modelos predictivos aplicables a contextos reales de banca, tanto en **gesti√≥n de patrimonio** como en **evaluaci√≥n de riesgo**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c73f69",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ **Importaci√≥n de librer√≠as y carga del dataset**\n",
    "\n",
    "Incluye todas las librer√≠as necesarias para manipular datos, visualizar resultados y entrenar modelos.  \n",
    "Se carga tambi√©n el fichero `bank-full.csv` y se muestran sus primeras filas para verificar que se ha importado correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa6f018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulaci√≥n y an√°lisis de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io # Agregamos io para el manejo de strings como archivos\n",
    "\n",
    "# Visualizaci√≥n\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocesamiento y modelos (scikit-learn)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Modelos que usaremos m√°s adelante\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# --- Modelos de Regresi√≥n Lineal ---\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "# --- Modelos Basados en √Årboles ---\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# --- M√©tricas de Evaluaci√≥n ---\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# M√©tricas\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6208401",
   "metadata": {},
   "source": [
    "Cargamos el dataset y visualizamos las primeras filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9cbd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = './data/bank-full.csv'\n",
    "\n",
    "# 1. Leer el archivo completo como texto\n",
    "try:\n",
    "    with open(dataset, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "except FileNotFoundError:\n",
    "    # Manejo de error si el archivo no est√° en la ubicaci√≥n esperada\n",
    "    print(f\"Error: No se encontr√≥ el archivo '{dataset}'. Aseg√∫rate de que est√° cargado correctamente.\")\n",
    "    exit()\n",
    "\n",
    "# 2. Modificaci√≥n solicitada: Reemplazar todas las comillas dobles (\") con una cadena vac√≠a\n",
    "# Esto limpia tanto el encabezado como el resto de los datos.\n",
    "cleaned_content = content.replace('\"', '')\n",
    "\n",
    "# 3. Leer el DataFrame usando io.StringIO\n",
    "# io.StringIO permite a Pandas leer la cadena de texto como si fuera un archivo.\n",
    "df = pd.read_csv(io.StringIO(cleaned_content), sep=';')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27ac547",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ **Exploraci√≥n inicial del dataset (EDA b√°sico)**\n",
    "\n",
    "Este apartado da las primeras pistas sobre posibles problemas o particularidades del dataset.\n",
    "\n",
    "- .info(), .describe(), .nunique()\n",
    "- Dimensiones, tipos de datos, distribuci√≥n inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163e69ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensiones del dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2781457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informaci√≥n general del dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e601ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estad√≠sticas generales\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20920d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N√∫mero de valores √∫nicos por columna\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0946fcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisamos la distribuci√≥n inicial de la variable objetivo para saber si el dataset est√° desbalanceado\n",
    "df['balance'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7249e64",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ **Limpieza de datos**\n",
    "En esta parte se revisa y corrige la calidad de los datos:\n",
    "- Detecci√≥n de valores nulos  \n",
    "- Comprobaci√≥n de categor√≠as inconsistentes  \n",
    "- Revisi√≥n de posibles valores at√≠picos extremos que distorsionen el an√°lisis  \n",
    "El prop√≥sito es garantizar que el dataset sea adecuado para construir modelos fiables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445b3220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisamos los valores nulos que pueda tener el dataset\n",
    "print(\"\\n--- VALORES NULOS ---\")\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18bccf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- VALORES DUPLICADOS ---\")\n",
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a174351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAMBIAMOS EL NOMBRE A LA VARIABLE 'y' QUE ERA LA VARIABLE OBJETIVO ORIGINAL DEL DATASET\n",
    "# Definimos el cambio de nombre\n",
    "rename_map = {\n",
    "    'y': 'subscribed_term_deposit'\n",
    "}\n",
    "\n",
    "# Aplicamos el cambio de nombre\n",
    "df = df.rename(columns=rename_map)\n",
    "\n",
    "# Mostramos las primeras filas y los nombres de las columnas para verificar\n",
    "print(\"Nombres de columnas despu√©s de renombrar:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b47136e",
   "metadata": {},
   "source": [
    "# BORRAR COLUMNAS QUE APORTEN RUIDO A LOS MODELOS, EN LA EXPLICACI√ìN PONER QUE SE HAN HECHO PRUEBAS CON ESAS COLUMNAS Y SIN ELLAS PARA DEMOSTRAR QUE NO APORTAN NADA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b00ae1",
   "metadata": {},
   "source": [
    "# Dataframe con columnas borradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d348aa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "\n",
    "df2.drop(columns=['duration', 'day', 'pdays', 'previous', 'poutcome', 'contact'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36158fe6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a7d75be",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ **An√°lisis profundo del variable `balance` (variable clave del proyecto)**\n",
    "La columna `balance` es el pilar de las dos variables objetivo.  \n",
    "En este apartado se debe:\n",
    "- Analizar la distribuci√≥n del balance  \n",
    "- Identificar valores negativos, picos extra√±os o colas largas  \n",
    "- Calcular rangos, percentiles clave (P50, P75, P90, P95)  \n",
    "- Visualizarlo con histogramas, boxplots o KDE  \n",
    "Este an√°lisis justifica los umbrales propuestos para los dos escenarios financieros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dcf61d",
   "metadata": {},
   "source": [
    "### 5.1. Revisi√≥n inicial y distribuci√≥n general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33b2054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero verificamos el tipo de la columna 'balance' y visualizamos los primeros valores\n",
    "print(\"---------- TIPO DE LA COLUMNA BALANCE ----------\")\n",
    "print(df['balance'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec20a07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------- MOSTRAMOS LAS 5 PRIMERAS FILAS DE LA COLUMNA BALANCE ----------\")\n",
    "df['balance'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b52e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------- MOSTRAMOS DATOS DESCRIPTIVOS DE LA COLUMNA BALANCE\")\n",
    "df['balance'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97c7f2f",
   "metadata": {},
   "source": [
    "#### ¬øQu√© conclusiones podemos sacar con estos valores?\n",
    "\n",
    "Al analizar la columna balance, vemos una realidad financiera bastante desigual entre los clientes del banco. Para empezar, encontramos balances negativos que llegan hasta ‚Äì8.019 ‚Ç¨, lo que nos confirma que hay personas que est√°n en descubierto o arrastran deudas importantes. Esto ya nos da una pista clara: hay un grupo de clientes que podr√≠a considerarse en una situaci√≥n de riesgo, y merece la pena calcular qu√© parte del dataset se encuentra en n√∫meros rojos.\n",
    "\n",
    "Si miramos los percentiles, tambi√©n queda bastante claro que la mayor√≠a de los clientes no tiene grandes cantidades de dinero en la cuenta. El 25% no llega ni a 72 ‚Ç¨, la mitad se queda alrededor de los 448 ‚Ç¨, y el 75% est√° por debajo de 1.428 ‚Ç¨. Dicho de forma sencilla: la gran mayor√≠a de los clientes tiene menos de 1.500 ‚Ç¨ en balance, lo que apunta a perfiles con poco margen financiero y un nivel de ahorro limitado. Adem√°s, como la mediana (448 ‚Ç¨) es muy inferior a la media (1.362 ‚Ç¨), podemos intuir que hay unos pocos clientes con mucho dinero que est√°n tirando del promedio hacia arriba.\n",
    "\n",
    "Y esto se confirma cuando miramos el valor m√°ximo: 102.127 ‚Ç¨, que es una cifra enorme comparada con lo que tiene la mayor√≠a. Esto nos muestra una distribuci√≥n con una cola derecha muy larga, t√≠pica cuando unos pocos clientes acumulan mucho capital. Este peque√±o grupo de ‚Äúgrandes balances‚Äù hace que la desviaci√≥n est√°ndar sea muy alta (3.044 ‚Ç¨), reflejando que las diferencias entre clientes son bastante grandes.\n",
    "\n",
    "En resumen, el comportamiento del balance no es uniforme ni equilibrado: hay muchas personas con poco dinero o incluso en negativo, y unas pocas con cantidades muy altas. Por eso, para definir umbrales que clasifiquen a los clientes (por ejemplo: ‚Äúriesgo‚Äù, ‚Äúestable‚Äù, ‚Äúalto balance‚Äù), tiene m√°s sentido apoyarse en percentiles que en la media. La mediana y los percentiles superiores (como P90 o P95) cuentan una historia mucho m√°s fiel de lo que realmente ocurre en la base de clientes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700696e0",
   "metadata": {},
   "source": [
    "### **5.2. Analizar espec√≠ficamente los valores negativos**\n",
    "\n",
    "En este apartado lo que queremos es:\n",
    "\n",
    "- Saber cu√°ntos clientes est√°n en negativo\n",
    "- Calcular qu√© porcentaje representan\n",
    "- Ver hasta qu√© punto pueden ser negativos\n",
    "- Entender si hay valores extremos (outliers negativos)\n",
    "- Detectar si est√°n agrupados o dispersos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adab7974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero filtramos los valores negativos\n",
    "negativos = df[df['balance'] < 0]\n",
    "\n",
    "# Calculamos el total de valores negativos\n",
    "cantidad_negativos = len(negativos)\n",
    "print(f\"Cantidad total de filas con un 'balance' negativo: {cantidad_negativos}\")\n",
    "\n",
    "# Calculamos el porcentaje que representa sobre el total\n",
    "porcentaje_negativos = (cantidad_negativos / len(df)) * 100\n",
    "print(f\"Porcentaje de negativos en el dataset: {round(porcentaje_negativos, 4)}%\")\n",
    "\n",
    "# Miramos tambi√©n cu√°l es el valor m√≠nimo y el m√°ximo dentro del grupo de valores negativos\n",
    "min_negativo = negativos['balance'].min()\n",
    "max_negativo = negativos['balance'].max()\n",
    "\n",
    "print(f\"Valor m√≠nimo dentro del grupo de negativos: {min_negativo}\")\n",
    "print(f\"Valor m√°ximo dentro del grupo de negativos: {max_negativo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7a4d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos con un gr√°fico la distribuci√≥n de valores negativos para visualizar si los valores negativos se concentran cerca de cero o si hay muchos casos extremos muy negativos\n",
    "plt.figure(figsize=(20, 4))\n",
    "\n",
    "sns.violinplot(\n",
    "    x=negativos['balance'],\n",
    "    orient='h',\n",
    "    inner='quartile',\n",
    "    color='#6EC6B0',      # color m√°s profesional (teal suave)\n",
    "    linewidth=1.2,        # borde m√°s definido\n",
    "    cut=0\n",
    ")\n",
    "\n",
    "plt.title(\"Distribuci√≥n de balances negativos\")\n",
    "plt.xlabel(\"Balance (‚Ç¨)\")\n",
    "\n",
    "# Crear ticks de 0 a -8000 cada 500\n",
    "plt.xticks(\n",
    "    ticks=range(0, -8500, -500), \n",
    "    labels=[str(t) for t in range(0, -8500, -500)]\n",
    ")\n",
    "\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada63cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "\n",
    "sns.violinplot(\n",
    "    x=negativos['balance'],\n",
    "    orient='h',\n",
    "    inner='quartile',\n",
    "    color='#6EC6B0',      # color m√°s profesional (teal suave)\n",
    "    linewidth=1.2,        # borde m√°s definido\n",
    "    cut=0\n",
    ")\n",
    "\n",
    "plt.title(\"Distribuci√≥n de balances negativos (Zoom 0 a ‚Äì1500)\", fontsize=14)\n",
    "plt.xlabel(\"Balance (‚Ç¨)\", fontsize=12)\n",
    "\n",
    "# Crear ticks de 0 a -1500 cada 500 (zona donde est√° el 90% de los valores negativos)\n",
    "ticks = range(0, -1600, -100)\n",
    "labels = [f\"{t} ‚Ç¨\" if t == 0 else f\"‚Äì{abs(t)} ‚Ç¨\" for t in ticks]\n",
    "\n",
    "plt.xticks(\n",
    "    ticks=ticks,\n",
    "    labels=labels,\n",
    "    fontsize=11\n",
    ")\n",
    "\n",
    "# Activamos grid principal\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.6, linewidth=0.7)\n",
    "\n",
    "# Activamos grid menor para m√°s gu√≠a visual\n",
    "plt.minorticks_on()\n",
    "plt.grid(axis='x', which='minor', linestyle=':', alpha=0.3)\n",
    "\n",
    "# Zoom en la parte donde se concentra el 90% de los valores negativos\n",
    "plt.xlim(-1500, 0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353abbe2",
   "metadata": {},
   "source": [
    "### **¬øConclusiones?**\n",
    "\n",
    "Los dos gr√°ficos muestran que la mayor√≠a de los clientes que est√°n en negativo lo est√°n por poco. La mayor concentraci√≥n aparece cerca de 0 ‚Ç¨, especialmente entre 0 y ‚Äì300 ‚Ç¨, lo que indica que la mayor√≠a de los descubiertos son peque√±os y probablemente temporales.\n",
    "\n",
    "A medida que el balance se vuelve m√°s negativo, la distribuci√≥n se va estrechando, lo que refleja que cada vez hay menos clientes con deudas medias (entre ‚Äì500 ‚Ç¨ y ‚Äì1500 ‚Ç¨). Son casos menos frecuentes y bastante dispersos.\n",
    "\n",
    "En el gr√°fico completo se aprecia claramente que los valores extremadamente negativos existen pero son muy poco comunes. La enorme diferencia entre la concentraci√≥n cercana a cero y la cola larga hacia ‚Äì8000 ‚Ç¨ confirma que los descubiertos profundos son excepcionales y que el grupo principal de clientes en negativo tiene un perfil de riesgo mucho m√°s moderado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3b2135",
   "metadata": {},
   "source": [
    "### 5.3. Calcular percentiles clave (P50, P75, P90 y P95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e70afdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== C√°lculo de percentiles clave ====\n",
    "p50 = np.percentile(df['balance'], 50)\n",
    "p75 = np.percentile(df['balance'], 75)\n",
    "p90 = np.percentile(df['balance'], 90)\n",
    "p95 = np.percentile(df['balance'], 95)\n",
    "\n",
    "print(\"Percentil 50 (mediana):\", p50)\n",
    "print(\"Percentil 75:\", p75)\n",
    "print(\"Percentil 90:\", p90)\n",
    "print(\"Percentil 95:\", p95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a1b9cb",
   "metadata": {},
   "source": [
    "Tras calcular los percentiles clave de la columna balance, observamos que el 50% de los clientes tiene 448 ‚Ç¨ o menos, lo que describe claramente al cliente promedio del banco. El percentil 75 se sit√∫a en torno a 1428 ‚Ç¨, indicando que solo una cuarta parte del conjunto supera este nivel.\n",
    "\n",
    "Los percentiles altos (P90 ‚âà 3574 ‚Ç¨ y P95 ‚âà 5768 ‚Ç¨) confirman que los clientes con balances altos representan un porcentaje muy reducido.\n",
    "\n",
    "En la banca real, no todos los clientes son iguales. El 10% de los clientes m√°s ricos a menudo generan el 80% de los ingresos de un banco (\"Principio de Pareto\"), por lo que elegiremos utilizar el valor del percentil 90."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070e1de0",
   "metadata": {},
   "source": [
    "### **5.4. Representaci√≥n gr√°fica mediante Histograma, KDE y Boxplot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96808776",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "sns.histplot(df['balance'], bins=60, kde=True, color='#4A90E2')\n",
    "plt.title(\"Distribuci√≥n completa de la variable balance\")\n",
    "plt.xlabel(\"Balance (‚Ç¨)\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9317d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "sns.histplot(\n",
    "    df[(df['balance'] >= -3000) & (df['balance'] <= 3000)]['balance'],\n",
    "    bins=60,\n",
    "    kde=True,\n",
    "    color='#7E57C2'\n",
    ")\n",
    "\n",
    "plt.title(\"Distribuci√≥n del balance (Zoom ‚Äì3000‚Ç¨ a 3000‚Ç¨)\")\n",
    "plt.xlabel(\"Balance (‚Ç¨)\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.4)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513293f3",
   "metadata": {},
   "source": [
    "# **ESTA PARTE HAY QUE REVISAR LOS GR√ÅFICOS QUE NO ME CONVENCEN MUCHO**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7157baf9",
   "metadata": {},
   "source": [
    "### ¬øConclusiones?\n",
    "\n",
    "# FASFSASGFASGAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa22d1d5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ **An√°lisis exploratorio de variables financieras y de riesgo**\n",
    "\n",
    "- Distribuci√≥n de edades\n",
    "- Clientes con default (impagos)\n",
    "- Pr√©stamos personales (loan)\n",
    "- Personas con hipoteca (housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efdd59c",
   "metadata": {},
   "source": [
    "### **6.1. Distribuci√≥n de edad de la cartera de clientes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab43093",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(df[\"age\"], kde=True, bins=30)\n",
    "plt.title(\"Distribuci√≥n de la edad de los clientes\")\n",
    "plt.xlabel(\"Edad\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.show()\n",
    "\n",
    "# Boxplot para ver outliers\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.boxplot(x=df[\"age\"])\n",
    "plt.title(\"Boxplot de la edad\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aa0a6a",
   "metadata": {},
   "source": [
    "### **6.2. Distribuci√≥n de clientes con impagos anteriores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c519cdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(data=df, x=\"default\")\n",
    "plt.title(\"Clientes con impagos previos (default)\")\n",
    "plt.xlabel(\"default\")\n",
    "plt.ylabel(\"Cantidad\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146b1a2b",
   "metadata": {},
   "source": [
    "### **6.3. Distribuci√≥n de clientes con pr√©stamos personales**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3025b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(data=df, x=\"loan\", palette=\"viridis\")\n",
    "plt.title(\"¬øEl cliente tiene pr√©stamo personal?\")\n",
    "# plt.xlabel(\"¬øTienes un pr√©stamo\")\n",
    "plt.ylabel(\"Cantidad de clientes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf49934",
   "metadata": {},
   "source": [
    "### **6.4. Distribuci√≥n de clientes que tienen una hipoteca en curso**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9476975",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(data=df, x=\"housing\")\n",
    "plt.title(\"Clientes con pr√©stamo de vivienda\")\n",
    "# plt.xlabel(\"housing\")\n",
    "plt.ylabel(\"Cantidad\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ac33fd",
   "metadata": {},
   "source": [
    "### **6.5. Matriz de correlaci√≥n de variables num√©ricas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b079ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = df.copy()\n",
    "\n",
    "df_encoded = df_encoded.drop(columns=['pdays', 'contact', 'month', 'day', 'poutcome', 'job', 'duration', 'campaign', 'previous'])\n",
    "\n",
    "df_encoded = pd.get_dummies(df_encoded, columns=['marital', 'education', 'default', 'housing', 'loan', 'subscribed_term_deposit'], drop_first=True)\n",
    "\n",
    "df_encoded.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc7fa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos la matriz de correlaci√≥n\n",
    "corr_matrix = df_encoded.corr()\n",
    "\n",
    "# Graficamos el heatmap\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(\n",
    "    corr_matrix, \n",
    "    annot=True,      # Muestra los n√∫meros (coeficientes) en cada celda\n",
    "    fmt='.2f',       # Formatea los n√∫meros a 2 decimales\n",
    "    cmap='coolwarm',\n",
    "    linewidths=0.5\n",
    ")\n",
    "plt.title('Matriz de Correlaci√≥n de Variables Relevantes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a61f8b8",
   "metadata": {},
   "source": [
    "## **CONCLUSIONES**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38315d9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7Ô∏è‚É£ **Creaci√≥n de los dos escenarios (targets)**\n",
    "\n",
    "En ambos casos se debe analizar qu√© proporci√≥n de clientes pertenece a cada grupo y si es necesario aplicar t√©cnicas para tratar desbalances.\n",
    "\n",
    "### üü© Escenario A: `Patrimonio_Alto`\n",
    "##### 1. Crear la variable binaria seg√∫n el umbral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e50247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la variable binaria 'Patrimonio_Alto'\n",
    "\n",
    "# La variable podr√° tener el valor 1 si el balance es mayor que el umbral, 0 si no lo es.\n",
    "df[\"Patrimonio_Alto\"] = (df[\"balance\"] > p90).astype(int)\n",
    "\n",
    "# Comprobar la creaci√≥n de la nueva columna\n",
    "print(\"Primeras filas con la nueva variable 'Patrimonio_Alto':\")\n",
    "print(df[['balance', 'Patrimonio_Alto']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad47b5ef",
   "metadata": {},
   "source": [
    "##### 2. Comprobar cu√°ntos registros quedan en cada clase (balanceadas/desbalanceadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47244219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar el n√∫mero de registros en cada clase\n",
    "conteo_clases = df['Patrimonio_Alto'].value_counts()\n",
    "proporcion_clases = df['Patrimonio_Alto'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"\\nResultados del An√°lisis de Desbalance de 'Patrimonio_Alto':\")\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(f\"Conteo de Clases:\\n{conteo_clases}\")\n",
    "print(\"\\nProporci√≥n de Clases:\")\n",
    "print(proporcion_clases.map('{:.2f}%'.format))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f80f39",
   "metadata": {},
   "source": [
    "Gracias a esto observamos un dataset claramente desbalanceado, como era de esperar porque P90 define al 10% superior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45c51e9",
   "metadata": {},
   "source": [
    "### üü• Escenario B: `Riesgo_Sobregiro`\n",
    "- Aplicar el umbral cero  \n",
    "- Crear la variable binaria correspondiente  \n",
    "- Revisar la distribuci√≥n resultante \n",
    "\n",
    "##### 1. Aplicar el umbral cero y crear la variable binaria seg√∫n el umbral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bc5ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el umbral 0\n",
    "umbral_cero = 0\n",
    "\n",
    "# Creamos la variable binaria 'Riesgo_Sobregiro'\n",
    "# 1 si el balance es menor que cero (riesgo/sobregiro), 0 si es cero o positivo.\n",
    "df['Riesgo_Sobregiro'] = np.where(df['balance'] < umbral_cero, 1, 0)\n",
    "\n",
    "# Comprobamos la creaci√≥n de la nueva columna\n",
    "print(\"Primeras filas con la nueva variable 'Riesgo_Sobregiro':\")\n",
    "print(df[['balance', 'Riesgo_Sobregiro']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea8ab7f",
   "metadata": {},
   "source": [
    "##### 2. Comprobar cu√°ntos registros quedan en cada clase (balanceadas/desbalanceadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b58430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar el n√∫mero de registros en cada clase\n",
    "conteo_clases_riesgo = df['Riesgo_Sobregiro'].value_counts()\n",
    "proporcion_clases_riesgo = df['Riesgo_Sobregiro'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"\\nResultados del An√°lisis de Desbalance de 'Riesgo_Sobregiro':\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(f\"Conteo de Clases:\\n{conteo_clases_riesgo}\")\n",
    "print(\"\\nProporci√≥n de Clases:\")\n",
    "print(proporcion_clases_riesgo.map('{:.2f}%'.format))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32e2c98",
   "metadata": {},
   "source": [
    "### ¬øConclusiones?\n",
    "\n",
    "Al crear el escenario **Patrimonio_Alto**, observamos que √∫nicamente el 10% de los clientes supera el umbral del percentil 90 del balance. Esto confirma que el grupo de clientes con un nivel econ√≥mico claramente elevado es muy reducido en comparaci√≥n con el resto de la base. La distribuci√≥n est√° fuertemente desbalanceada, lo que significa que cualquier modelo que utilice esta variable necesitar√° tener en cuenta este desequilibrio para no favorecer en exceso a la clase mayoritaria.\n",
    "\n",
    "En el escenario **Riesgo_Sobregiro**, los resultados muestran que aproximadamente el 8.33% de los clientes tiene saldo negativo, mientras que el 91.67% mantiene un balance igual o superior a cero. Aunque este escenario tambi√©n presenta desbalance, la diferencia entre clases es ligeramente menor que en el caso del patrimonio alto. Aun as√≠, sigue siendo importante considerar t√©cnicas de balanceo si se desea entrenar modelos de clasificaci√≥n consistentes.\n",
    "\n",
    "En conjunto, ambos escenarios revelan una estructura muy desigual en los extremos financieros de la cartera: solo una minor√≠a acumula altos niveles de patrimonio y una minor√≠a diferente incurre en sobregiro. Esto ofrece oportunidades interesantes para el modelado, pero tambi√©n exige atenci√≥n especial a la desproporci√≥n entre clases que podr√≠a afectar a futuros algoritmos de machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748fb3db",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8Ô∏è‚É£ **Preprocesamiento para Machine Learning**\n",
    "Aqu√≠ se preparan los datos antes de entrenar modelos:\n",
    "- Identificar variables categ√≥ricas y num√©ricas  \n",
    "- Crear transformadores:  \n",
    "  - `OneHotEncoder` para categor√≠as  \n",
    "  - `StandardScaler` para variables num√©ricas  \n",
    "- Separar en train/test  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00f94cc",
   "metadata": {},
   "source": [
    "#### 1. Separar datos en X e Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60ec79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['Patrimonio_Alto', 'Riesgo_Sobregiro', 'balance'])\n",
    "\n",
    "# Variable objetivo para el escenario A\n",
    "y_escenario_A = df['Patrimonio_Alto']\n",
    "\n",
    "# Variable objetivo para el escenario B\n",
    "y_escenario_B = df['Riesgo_Sobregiro']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e27bc3",
   "metadata": {},
   "source": [
    "X e Y dataframe con columnas eliminadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590a554f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = df2.drop(columns=['Patrimonio_Alto', 'Riesgo_Sobregiro', 'balance'])\n",
    "\n",
    "# Variable objetivo para el escenario A\n",
    "y_escenario_A2 = df2['Patrimonio_Alto']\n",
    "\n",
    "# Variable objetivo para el escenario B\n",
    "y_escenario_B = df2['Riesgo_Sobregiro']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaee758",
   "metadata": {},
   "source": [
    "#### 2. Identificar variables categ√≥ricas y num√©ricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61f0cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "cat_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "print(\"Num√©ricas:\", list(num_features))\n",
    "print(\"Categ√≥ricas:\", list(cat_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591ccb1b",
   "metadata": {},
   "source": [
    "Numericas y categoricas para dataframe con columnas eliminadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679d1c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = X2.select_dtypes(include=['int64', 'float64']).columns\n",
    "cat_features = X2.select_dtypes(include=['object']).columns\n",
    "\n",
    "print(\"Num√©ricas:\", list(num_features))\n",
    "print(\"Categ√≥ricas:\", list(cat_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f05e876",
   "metadata": {},
   "source": [
    "#### 3. Crear transformadores\n",
    "\n",
    "Para las categ√≥ricas usaremos OneHotEncoder y para las num√©ricas StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298e4193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categ√≥ricas\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Num√©ricas\n",
    "numeric_transformer = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b698338",
   "metadata": {},
   "source": [
    "#### 4. Crear el ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3589c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_features),\n",
    "        ('cat', categorical_transformer, cat_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a2f8ae",
   "metadata": {},
   "source": [
    "## üü© Escenario A ‚Äî `Patrimonio_Alto`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eba81fd",
   "metadata": {},
   "source": [
    "#### 5. Dividir en train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef63a7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_A, X_test_A, y_train_A, y_test_A = train_test_split(\n",
    "    X,\n",
    "    y_escenario_A,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_escenario_A\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6b725c",
   "metadata": {},
   "source": [
    "#### 6. Aplicar el preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f45c7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Ajustar el preprocesador solo con los datos de entrenamiento\n",
    "X_train_A_preprocessed = preprocessor.fit_transform(X_train_A)\n",
    "\n",
    "# Transformar los datos de test con el mismo ajuste\n",
    "X_test_A_preprocessed = preprocessor.transform(X_test_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9375ca62",
   "metadata": {},
   "source": [
    "REVISAR SE UTILIZA PARA EQUILIBRAR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a37f7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar SMOTE para balancear las clases en el conjunto de entrenamiento\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train_A_preprocessed, y_train_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0080cc9",
   "metadata": {},
   "source": [
    "SMOTE\n",
    "\n",
    "Solo debes aplicarlo al conjunto de entrenamiento, nunca al test.\n",
    "\n",
    "Funciona solo con variables num√©ricas ‚Üí si tienes variables categ√≥ricas, deben estar ya codificadas (OneHotEncoder, LabelEncoder, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c8771c",
   "metadata": {},
   "source": [
    "## üü• Escenario B: `Riesgo_Sobregiro`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54cd7d1",
   "metadata": {},
   "source": [
    "#### 5. Dividir en train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49fb86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_B, X_test_B, y_train_B, y_test_B = train_test_split(\n",
    "    X,\n",
    "    y_escenario_B,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_escenario_B\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfac406b",
   "metadata": {},
   "source": [
    "#### 6. Aplicar el preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c20184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar el preprocesador solo con los datos de entrenamiento\n",
    "X_train_B_preprocessed = preprocessor.fit_transform(X_train_B)\n",
    "\n",
    "# Transformar los datos de test con el mismo ajuste\n",
    "X_test_B_preprocessed = preprocessor.transform(X_test_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493ce8e8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9Ô∏è‚É£ **Entrenamiento de modelos de clasificaci√≥n**\n",
    "Se entrenan modelos como:\n",
    "- Regresi√≥n Log√≠stica  \n",
    "- √Årbol de Decisi√≥n  \n",
    "- Random Forest  \n",
    "- KNN  \n",
    "- Naive Bayes  \n",
    "- SVM  \n",
    "- MLP (Red Neuronal sencilla)\n",
    "\n",
    "El enfoque es comparativo: ver cu√°l funciona mejor seg√∫n las m√©tricas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862afad8",
   "metadata": {},
   "source": [
    "## üü© Escenario A ‚Äî `Patrimonio_Alto`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381146ef",
   "metadata": {},
   "source": [
    "### Regresi√≥n log√≠stica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c148baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_a = LogisticRegression(max_iter=1500)\n",
    "\n",
    "inicio = time.time()\n",
    "rl_a.fit(X_train_A_preprocessed, y_train_res)\n",
    "fin = time.time()\n",
    "\n",
    "# Realiza predicciones\n",
    "y_pred_lr = rl_a.predict(X_test_A_preprocessed)\n",
    "\n",
    "tiempo_rl_a = fin - inicio\n",
    "print(\"Tiempo Logistic Regression (Escenario A):\")\n",
    "print(f\"{tiempo_rl_a:.2f}\")\n",
    "\n",
    "print(classification_report(y_test_A, y_pred_lr))\n",
    "\n",
    "# Calcula la matriz de confusi√≥n\n",
    "cm_lr = confusion_matrix(y_test_A, y_pred_lr)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_lr, display_labels=['Clase media/baja (0)', 'Clase alta (1)'])\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Matriz de Confusi√≥n - Regresi√≥n Log√≠stica\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1914fb7",
   "metadata": {},
   "source": [
    "#### Modificado para que este m√°s equilibrado y no salga tan mal por estar tan desbalanceado REVISAR SI SE PUEDE MEJORAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaefb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rl_a = LogisticRegression(max_iter=1500)\n",
    "\n",
    "inicio = time.time()\n",
    "rl_a.fit(X_train_res, y_train_res)\n",
    "fin = time.time()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# CALCULAR EL UMBRAL √ìPTIMO\n",
    "# Encontrar el umbral que maximiza el F1 de la clase 1.\n",
    "y_proba = rl_a.predict_proba(X_test_A_preprocessed)[:, 1]\n",
    "\n",
    "best_f1 = 0\n",
    "best_threshold = 0\n",
    "\n",
    "for t in np.arange(0.01, 1.0, 0.01):\n",
    "    y_pred_t = (y_proba > t).astype(int)\n",
    "    f1 = f1_score(y_test_A, y_pred_t)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = t\n",
    "\n",
    "print(\"Mejor umbral:\", best_threshold)\n",
    "print(\"Mejor F1:\", best_f1)\n",
    "\n",
    "\n",
    "# Realiza predicciones\n",
    "y_pred_lr = (y_proba > best_threshold).astype(int)\n",
    "\n",
    "tiempo_rl_a = fin - inicio\n",
    "print(\"Tiempo Logistic Regression (Escenario A):\")\n",
    "print(f\"{tiempo_rl_a:.2f}\")\n",
    "\n",
    "print(classification_report(y_test_A, y_pred_lr))\n",
    "\n",
    "# Calcula la matriz de confusi√≥n\n",
    "cm_lr = confusion_matrix(y_test_A, y_pred_lr)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_lr, display_labels=['Clase media/baja (0)', 'Clase alta (1)'])\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Matriz de Confusi√≥n - Regresi√≥n Log√≠stica\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac02bfd0",
   "metadata": {},
   "source": [
    "### √Årbol de decisi√≥n optimizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc28c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_opt_a = DecisionTreeClassifier()\n",
    "\n",
    "# Definir el diccionario de par√°metros a probar\n",
    "param_grid = {\n",
    "    'max_depth': [None, 3, 5, 7, 10, 15, 20],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 4, 6, 8],\n",
    "    'max_features': [None, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Configurar GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=tree_opt_a,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # validaci√≥n cruzada con 5 particiones\n",
    "    scoring='f1',\n",
    "    n_jobs=-1  # usa todos los n√∫cleos disponibles para acelerar\n",
    ")\n",
    "\n",
    "# Entrenar el GridSearchCV\n",
    "inicio = time.time()\n",
    "grid_search.fit(X_train_A_preprocessed, y_train_A)\n",
    "fin = time.time()\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Obtener el mejor modelo\n",
    "best_model_dt = grid_search.best_estimator_\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred_dt_opt = best_model_dt.predict(X_test_A_preprocessed)\n",
    "\n",
    "tiempo_dt_a = fin - inicio\n",
    "print(\"Tiempo Decision Tree Optimizado (Escenario A):\")\n",
    "print(f\"{tiempo_dt_a:.2f}\")\n",
    "\n",
    "print(classification_report(y_test_A, y_pred_dt_opt))\n",
    "\n",
    "cm_dt_opt = confusion_matrix(y_test_A, y_pred_dt_opt)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_dt_opt)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Matriz de Confusi√≥n - Regresi√≥n Log√≠stica\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4307aff3",
   "metadata": {},
   "source": [
    "#### Modificado para que este m√°s equilibrado y no salga tan mal por estar tan desbalanceado REVISAR SI SE PUEDE MEJORAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4b7eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_opt_a = DecisionTreeClassifier()\n",
    "\n",
    "# Definir el diccionario de par√°metros a probar\n",
    "param_grid = {\n",
    "    'max_depth': [None, 3, 5, 7, 10, 15, 20],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 4, 6, 8],\n",
    "    'max_features': [None, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Configurar GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=tree_opt_a,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # validaci√≥n cruzada con 5 particiones\n",
    "    scoring='f1',\n",
    "    n_jobs=-1  # usa todos los n√∫cleos disponibles para acelerar\n",
    ")\n",
    "\n",
    "# Entrenar el GridSearchCV\n",
    "inicio = time.time()\n",
    "grid_search.fit(X_train_res, y_train_res)\n",
    "fin = time.time()\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Obtener el mejor modelo\n",
    "best_model_dt = grid_search.best_estimator_\n",
    "\n",
    "y_proba = best_model_dt.predict_proba(X_test_A_preprocessed)[:, 1]\n",
    "\n",
    "best_f1 = 0\n",
    "best_threshold = 0\n",
    "\n",
    "for t in np.arange(0.01, 1.0, 0.01):\n",
    "    y_pred_t = (y_proba > t).astype(int)\n",
    "    f1 = f1_score(y_test_A, y_pred_t)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = t\n",
    "\n",
    "print(\"Mejor umbral:\", best_threshold)\n",
    "print(\"Mejor F1:\", best_f1)\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred_dt_opt = (y_proba > best_threshold).astype(int)\n",
    "\n",
    "tiempo_dt_a = fin - inicio\n",
    "print(\"Tiempo Decision Tree Optimizado (Escenario A):\")\n",
    "print(f\"{tiempo_dt_a:.2f}\")\n",
    "\n",
    "print(classification_report(y_test_A, y_pred_dt_opt))\n",
    "\n",
    "cm_dt_opt = confusion_matrix(y_test_A, y_pred_dt_opt)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_dt_opt)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Matriz de Confusi√≥n - Regresi√≥n Log√≠stica\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa5b681",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b200d2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tree_a = RandomForestClassifier()\n",
    "\n",
    "inicio = time.time()\n",
    "random_tree_a.fit(X_train_A_preprocessed, y_train_A)\n",
    "fin = time.time()\n",
    "\n",
    "y_pred_rf = random_tree_a.predict(X_test_A_preprocessed)\n",
    "\n",
    "tiempo_rt_a = fin - inicio\n",
    "print(\"Tiempo Random Forest (Escenario A):\")\n",
    "print(f\"{tiempo_rt_a:.2f}\")\n",
    "\n",
    "print(classification_report(y_test_A, y_pred_rf))\n",
    "\n",
    "cm_rt = confusion_matrix(y_test_A, y_pred_rf)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_rt)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Matriz de Confusi√≥n - Random Forest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a5f3d2",
   "metadata": {},
   "source": [
    "#### Modificado para que este m√°s equilibrado y no salga tan mal por estar tan desbalanceado REVISAR SI SE PUEDE MEJORAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd840f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tree_a = RandomForestClassifier()\n",
    "\n",
    "inicio = time.time()\n",
    "random_tree_a.fit(X_train_res, y_train_res)\n",
    "fin = time.time()\n",
    "\n",
    "y_proba = random_tree_a.predict_proba(X_test_A_preprocessed)[:, 1]\n",
    "\n",
    "best_f1 = 0\n",
    "best_threshold = 0\n",
    "\n",
    "for t in np.arange(0.01, 1.0, 0.01):\n",
    "    y_pred_t = (y_proba > t).astype(int)\n",
    "    f1 = f1_score(y_test_A, y_pred_t)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = t\n",
    "\n",
    "y_pred_rf = (y_proba > best_threshold).astype(int)\n",
    "\n",
    "tiempo_rt_a = fin - inicio\n",
    "print(\"Tiempo Random Forest (Escenario A):\")\n",
    "print(f\"{tiempo_rt_a:.2f}\")\n",
    "\n",
    "print(classification_report(y_test_A, y_pred_rf))\n",
    "\n",
    "cm_rt = confusion_matrix(y_test_A, y_pred_rf)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_rt)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Matriz de Confusi√≥n - Random Forest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fbb0bc",
   "metadata": {},
   "source": [
    "### Random Forest Optimizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01ed201",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tree_opt = RandomForestClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Crear Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=random_tree_opt,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Ajustar modelo\n",
    "inicio = time.time()\n",
    "grid_search.fit(X_train_A_preprocessed, y_train_A)\n",
    "fin = time.time()\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Extraer mejor modelo\n",
    "best_model_rf = grid_search.best_estimator_\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred_rf_opt = best_model_rf.predict(X_test_A_preprocessed)\n",
    "\n",
    "tiempo_rf_opt_a = fin - inicio\n",
    "print(\"Tiempo Random Forest Optimizado (Escenario A):\")\n",
    "print(f\"{tiempo_rf_opt_a:.2f}\")\n",
    "\n",
    "print(classification_report(y_test_A, y_pred_rf_opt))\n",
    "\n",
    "cm_rf_opt = confusion_matrix(y_test_A, y_pred_rf_opt)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_rf_opt)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Matriz de Confusi√≥n - Random Forest Optimizado\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b288a8c0",
   "metadata": {},
   "source": [
    "#### Modificado para que este m√°s equilibrado y no salga tan mal por estar tan desbalanceado REVISAR SI SE PUEDE MEJORAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1587de69",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tree_opt = RandomForestClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Crear Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=random_tree_opt,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Ajustar modelo\n",
    "inicio = time.time()\n",
    "grid_search.fit(X_train_res, y_train_res)\n",
    "fin = time.time()\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Extraer mejor modelo\n",
    "best_model_rf = grid_search.best_estimator_\n",
    "\n",
    "y_proba = random_tree_a.predict_proba(X_test_A_preprocessed)[:, 1]\n",
    "\n",
    "best_f1 = 0\n",
    "best_threshold = 0\n",
    "\n",
    "for t in np.arange(0.01, 1.0, 0.01):\n",
    "    y_pred_t = (y_proba > t).astype(int)\n",
    "    f1 = f1_score(y_test_A, y_pred_t)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = t\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred_rf_opt = (y_proba > best_threshold).astype(int)\n",
    "\n",
    "tiempo_rf_opt_a = fin - inicio\n",
    "print(\"Tiempo Random Forest Optimizado (Escenario A):\")\n",
    "print(f\"{tiempo_rf_opt_a:.2f}\")\n",
    "\n",
    "print(classification_report(y_test_A, y_pred_rf_opt))\n",
    "\n",
    "cm_rf_opt = confusion_matrix(y_test_A, y_pred_rf_opt)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_rf_opt)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Matriz de Confusi√≥n - Random Forest Optimizado\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e220ca7c",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ace2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "\n",
    "inicio = time.time()\n",
    "knn.fit(X_train_A_preprocessed, y_train_A)\n",
    "fin = time.time()\n",
    "\n",
    "y_pred_knn = knn.predict(X_test_A_preprocessed)\n",
    "\n",
    "tiempo_knn_a = fin - inicio\n",
    "\n",
    "print(\"Tiempo KNN (Escenario A):\")\n",
    "print(f\"{tiempo_knn_a:.2f}\")\n",
    "\n",
    "print(classification_report(y_test_A, y_pred_knn))\n",
    "\n",
    "cm_knn = confusion_matrix(y_test_A, y_pred_knn)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_knn)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Matriz de Confusi√≥n - KNN\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3e363a",
   "metadata": {},
   "source": [
    "#### Modificado para que este m√°s equilibrado y no salga tan mal por estar tan desbalanceado REVISAR SI SE PUEDE MEJORAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3546d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "\n",
    "inicio = time.time()\n",
    "knn.fit(X_train_res, y_train_res)\n",
    "fin = time.time()\n",
    "\n",
    "y_proba = knn.predict_proba(X_test_A_preprocessed)[:, 1]\n",
    "\n",
    "best_f1 = 0\n",
    "best_threshold = 0\n",
    "\n",
    "for t in np.arange(0.01, 1.0, 0.01):\n",
    "    y_pred_t = (y_proba > t).astype(int)\n",
    "    f1 = f1_score(y_test_A, y_pred_t)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = t\n",
    "\n",
    "y_pred_knn = (y_proba > best_threshold).astype(int)\n",
    "\n",
    "tiempo_knn_a = fin - inicio\n",
    "\n",
    "print(\"Tiempo KNN (Escenario A):\")\n",
    "print(f\"{tiempo_knn_a:.2f}\")\n",
    "\n",
    "print(classification_report(y_test_A, y_pred_knn))\n",
    "\n",
    "cm_knn = confusion_matrix(y_test_A, y_pred_knn)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_knn)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Matriz de Confusi√≥n - KNN\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a9c756",
   "metadata": {},
   "source": [
    "### KNN Optimizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abb8851",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_opt = KNeighborsClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': [1, 5, 10, 15, 20],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "}\n",
    "\n",
    "# Crear Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=knn_opt,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Ajustar modelo\n",
    "inicio = time.time()\n",
    "grid_search.fit(X_train_A_preprocessed, y_train_A)\n",
    "fin = time.time()\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Extraer mejor modelo\n",
    "best_model_knn = grid_search.best_estimator_\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred_knn_opt = best_model_knn.predict(X_test_A_preprocessed)\n",
    "\n",
    "tiempo_knn_opt_a = fin - inicio\n",
    "print(\"Tiempo KNN Optimizado (Escenario A):\")\n",
    "print(f\"{tiempo_knn_opt_a:.2f}\")\n",
    "\n",
    "cm_knn_opt = confusion_matrix(y_test_A, y_pred_knn_opt)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_knn_opt)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Matriz de Confusi√≥n - KNN Optimizado\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29357f9a",
   "metadata": {},
   "source": [
    "#### Modificado para que este m√°s equilibrado y no salga tan mal por estar tan desbalanceado REVISAR SI SE PUEDE MEJORAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21f99dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_opt = KNeighborsClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': [1, 5, 10, 15, 20],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "}\n",
    "\n",
    "# Crear Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=knn_opt,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Ajustar modelo\n",
    "inicio = time.time()\n",
    "grid_search.fit(X_train_res, y_train_res)\n",
    "fin = time.time()\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Extraer mejor modelo\n",
    "best_model_knn = grid_search.best_estimator_\n",
    "\n",
    "y_proba = best_model_knn.predict_proba(X_test_A_preprocessed)[:, 1]\n",
    "\n",
    "best_f1 = 0\n",
    "best_threshold = 0\n",
    "\n",
    "for t in np.arange(0.01, 1.0, 0.01):\n",
    "    y_pred_t = (y_proba > t).astype(int)\n",
    "    f1 = f1_score(y_test_A, y_pred_t)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = t\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred_knn_opt = (y_proba > best_threshold).astype(int)\n",
    "\n",
    "tiempo_knn_opt_a = fin - inicio\n",
    "print(\"Tiempo KNN Optimizado (Escenario A):\")\n",
    "print(f\"{tiempo_knn_opt_a:.2f}\")\n",
    "\n",
    "cm_knn_opt = confusion_matrix(y_test_A, y_pred_knn_opt)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_knn_opt)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Matriz de Confusi√≥n - KNN Optimizado\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95242163",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a225e729",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = GaussianNB()\n",
    "\n",
    "inicio = time.time()\n",
    "nb.fit(X_train_A_preprocessed.toarray(), y_train_A)\n",
    "fin = time.time()\n",
    "\n",
    "y_pred_nb = nb.predict(X_test_A_preprocessed)\n",
    "\n",
    "tiempo_nb_a = fin - inicio\n",
    "print(\"Tiempo Naive Bayes (Escenario A):\")\n",
    "print(f\"{tiempo_nb_a:.2f}\")\n",
    "\n",
    "print(classification_report(y_test_A, y_pred_nb))\n",
    "\n",
    "cm_nb = confusion_matrix(y_test_A, y_pred_nb)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_nb)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Matriz de Confusi√≥n - Naive Bayes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703e865f",
   "metadata": {},
   "source": [
    "#### Modificado para que este m√°s equilibrado y no salga tan mal por estar tan desbalanceado REVISAR SI SE PUEDE MEJORAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ace6f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = GaussianNB()\n",
    "\n",
    "inicio = time.time()\n",
    "nb.fit(X_train_res.toarray(), y_train_res)\n",
    "fin = time.time()\n",
    "\n",
    "y_proba = nb.predict_proba(X_test_A_preprocessed.toarray())[:, 1]\n",
    "\n",
    "best_f1 = 0\n",
    "best_threshold = 0\n",
    "\n",
    "for t in np.arange(0.01, 1.0, 0.01):\n",
    "    y_pred_t = (y_proba > t).astype(int)\n",
    "    f1 = f1_score(y_test_A, y_pred_t)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = t\n",
    "\n",
    "y_pred_nb = (y_proba > best_threshold).astype(int)\n",
    "\n",
    "tiempo_nb_a = fin - inicio\n",
    "print(\"Tiempo Naive Bayes (Escenario A):\")\n",
    "print(f\"{tiempo_nb_a:.2f}\")\n",
    "\n",
    "print(classification_report(y_test_A, y_pred_nb))\n",
    "\n",
    "cm_nb = confusion_matrix(y_test_A, y_pred_nb)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_nb)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Matriz de Confusi√≥n - Naive Bayes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6e6436",
   "metadata": {},
   "source": [
    "### Naive Bayes Optimizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15795153",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_opt = GaussianNB()\n",
    "\n",
    "param_grid = {\n",
    "    'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4]\n",
    "}\n",
    "\n",
    "# Crear Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=nb_opt,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Ajustar modelo\n",
    "inicio = time.time()\n",
    "grid_search.fit(X_train_A_preprocessed, y_train_A)\n",
    "fin = time.time()\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Extraer mejor modelo\n",
    "best_model_nb = grid_search.best_estimator_\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred_nb_opt = best_model_nb.predict(X_test_A_preprocessed)\n",
    "\n",
    "tiempo_nb_opt_a = fin - inicio\n",
    "\n",
    "cm_nb_opt = confusion_matrix(y_test_A, y_pred_nb_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b98a05",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ce47ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "\n",
    "inicio = time.time()\n",
    "svc.fit(X_train_A_preprocessed, y_train_A)\n",
    "fin = time.time()\n",
    "\n",
    "y_pred_svc = svc.predict(X_test_A_preprocessed)\n",
    "\n",
    "tiempo_svc_a = fin - inicio\n",
    "\n",
    "cm_svc = confusion_matrix(y_test_A, y_pred_svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594b9096",
   "metadata": {},
   "source": [
    "### Support Vector Machines Optimizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ec400e",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_opt = SVC()\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Crear Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=svc_opt,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Ajustar modelo\n",
    "inicio = time.time()\n",
    "grid_search.fit(X_train_A_preprocessed, y_train_A)\n",
    "fin = time.time()\n",
    "\n",
    "# Extraer mejor modelo\n",
    "best_model_svc = grid_search.best_estimator_\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred_svc_opt = best_model_svc.predict(X_test_A_preprocessed)\n",
    "\n",
    "tiempo_svc_opt_a = fin - inicio\n",
    "\n",
    "cm_svc_opt = confusion_matrix(y_test_A, y_pred_svc_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5073e47e",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec371d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(random_state=42)\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "mlp.fit(X_train_A_preprocessed, y_train_A)\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred = mlp.predict(X_test_A_preprocessed)\n",
    "\n",
    "# Evaluar el rendimiento\n",
    "print(\"Matriz de confusi√≥n:\")\n",
    "print(confusion_matrix(y_test_A, y_pred))\n",
    "\n",
    "print(\"\\nReporte de clasificaci√≥n:\")\n",
    "print(classification_report(y_test_A, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e52d1ac",
   "metadata": {},
   "source": [
    "### MLP Optimizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249d3f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "hidden_layer_options = [\n",
    "    (50,),\n",
    "    (100,),\n",
    "    (100, 50),\n",
    "    (100, 50, 25),\n",
    "    (150, 75),\n",
    "    (200, 100, 50)\n",
    "]\n",
    "\n",
    "activation_options = ['relu', 'tanh']\n",
    "solver_options = ['adam']\n",
    "learning_rates = [0.001, 0.0005, 0.0001]\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# B√öSQUEDA DE TODAS LAS COMBINACIONES\n",
    "# -------------------------------------------------------------------\n",
    "best_f1 = 0\n",
    "best_params = None\n",
    "\n",
    "total_tests = len(hidden_layer_options) * len(activation_options) * len(solver_options) * len(learning_rates)\n",
    "print(f\"\\nProbando {total_tests} combinaciones...\\n\")\n",
    "\n",
    "for hls, act, solv, lr in itertools.product(hidden_layer_options, activation_options, solver_options, learning_rates):\n",
    "\n",
    "    print(f\"Probando: hidden={hls}, activation={act}, solver={solv}, lr={lr}\")\n",
    "\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=hls,\n",
    "        activation=act,\n",
    "        solver=solv,\n",
    "        learning_rate_init=lr,\n",
    "        max_iter=500,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    mlp.fit(X_train_A_preprocessed, y_train_A)\n",
    "    y_pred = mlp.predict(X_test_A_preprocessed)\n",
    "    f1 = f1_score(y_test_A, y_pred, pos_label=1)  # F1 para la clase 1\n",
    "\n",
    "    print(f\" ‚Üí F1 (clase 1): {f1:.4f}\\n\")\n",
    "\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_params = (hls, act, solv, lr)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# RESULTADOS FINALES\n",
    "# -------------------------------------------------------------------\n",
    "print(\"========================================\")\n",
    "print(\"MEJOR COMBINACI√ìN ENCONTRADA:\")\n",
    "print(\"Hidden layers:\", best_params[0])\n",
    "print(\"Activation:\", best_params[1])\n",
    "print(\"Solver:\", best_params[2])\n",
    "print(\"Learning rate:\", best_params[3])\n",
    "print(f\"F1 (clase 1): {best_f1:.4f}\")\n",
    "print(\"========================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6b3bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(150, 75),\n",
    "    activation='tanh',\n",
    "    solver='adam',\n",
    "    learning_rate_init=0.001,\n",
    "    max_iter=500,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "mlp.fit(X_train_A_preprocessed, y_train_A)\n",
    "\n",
    "y_pred_mlp_opt = mlp.predict(X_test_A_preprocessed)\n",
    "\n",
    "print(\"=== Classification Report ===\")\n",
    "print(classification_report(y_test_A, y_pred))\n",
    "\n",
    "# Matriz de confusi√≥n\n",
    "cm_mlp_opt = confusion_matrix(y_test_A, y_pred)\n",
    "\n",
    "sns.heatmap(cm_mlp_opt, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
    "plt.xlabel(\"Predicci√≥n\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.title(\"Matriz de confusi√≥n MLP\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7627ac",
   "metadata": {},
   "source": [
    "## üü• Escenario B: `Riesgo_Sobregiro`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba36296",
   "metadata": {},
   "source": [
    "### Regresi√≥n log√≠stica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c3c28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inicio = time.time()\n",
    "rl.fit(X_train_B_preprocessed, y_train_B)\n",
    "fin = time.time()\n",
    "\n",
    "# Realiza predicciones\n",
    "y_pred_lr_b = rl.predict(X_test_B_preprocessed)\n",
    "\n",
    "tiempo_rl_b = fin - inicio\n",
    "\n",
    "# Calcula la matriz de confusi√≥n\n",
    "cm_lr_b = confusion_matrix(y_test_B, y_pred_lr_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9bead9",
   "metadata": {},
   "source": [
    "### √Årbol de decisi√≥n optimizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089c8d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el diccionario de par√°metros a probar\n",
    "param_grid = {\n",
    "    'max_depth': [None, 3, 5, 7, 10, 15, 20],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 4, 6, 8],\n",
    "    'max_features': [None, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Configurar GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=tree_optimizado,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # validaci√≥n cruzada con 5 particiones\n",
    "    scoring='recall',\n",
    "    n_jobs=-1  # usa todos los n√∫cleos disponibles para acelerar\n",
    ")\n",
    "\n",
    "# Entrenar el GridSearchCV\n",
    "inicio = time.time()\n",
    "grid_search.fit(X_train_B_preprocessed, y_train_B)\n",
    "fin = time.time()\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Obtener el mejor modelo\n",
    "best_model_dt = grid_search.best_estimator_\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred_dt_opt_b = best_model_dt.predict(X_test_B_preprocessed)\n",
    "\n",
    "tiempo_dt_b = fin - inicio\n",
    "\n",
    "cm_dt_opt_b = confusion_matrix(y_test_B, y_pred_dt_opt_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a115e6",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cde27f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inicio = time.time()\n",
    "random_tree.fit(X_train_B_preprocessed, y_train_B)\n",
    "fin = time.time()\n",
    "\n",
    "y_pred_rf_b = random_tree.predict(X_test_B_preprocessed)\n",
    "\n",
    "tiempo_rt_b = fin - inicio\n",
    "\n",
    "cm_dt_b = confusion_matrix(y_test_B, y_pred_rf_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75957a4",
   "metadata": {},
   "source": [
    "### Random Forest Optimizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75eaf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Crear Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=random_tree_opt,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Ajustar modelo\n",
    "inicio = time.time()\n",
    "grid_search.fit(X_train_B_preprocessed, y_train_B)\n",
    "fin = time.time()\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Extraer mejor modelo\n",
    "best_model_rf = grid_search.best_estimator_\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred_rf_opt_b = best_model_rf.predict(X_test_B_preprocessed)\n",
    "\n",
    "tiempo_rf_opt_b = fin - inicio\n",
    "\n",
    "cm_rf_opt_b = confusion_matrix(y_test_B, y_pred_rf_opt_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a6d994",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5db7723",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "\n",
    "inicio = time.time()\n",
    "knn.fit(X_train_B_preprocessed, y_train_B)\n",
    "fin = time.time()\n",
    "\n",
    "y_pred_knn_b = knn.predict(X_test_B_preprocessed)\n",
    "\n",
    "tiempo_knn_b = fin - inicio\n",
    "\n",
    "cm_knn_b = confusion_matrix(y_test_B, y_pred_knn_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde63afc",
   "metadata": {},
   "source": [
    "### KNN Optimizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10528bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_neighbors': [1, 5, 10, 15, 20],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "}\n",
    "\n",
    "# Crear Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=knn_opt,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='recall',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Ajustar modelo\n",
    "inicio = time.time()\n",
    "grid_search.fit(X_train_B_preprocessed, y_train_B)\n",
    "fin = time.time()\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Extraer mejor modelo\n",
    "best_model_knn = grid_search.best_estimator_\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred_knn_opt_b = best_model_knn.predict(X_test_B_preprocessed)\n",
    "\n",
    "tiempo_knn_opt_b = fin - inicio\n",
    "\n",
    "cm_knn_opt = confusion_matrix(y_test_B, y_pred_knn_opt_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee3a15d",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1da62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inicio = time.time()\n",
    "nb.fit(X_train_B_preprocessed, y_train_B)\n",
    "fin = time.time()\n",
    "\n",
    "y_pred_nb_b = nb.predict(X_test_B_preprocessed)\n",
    "\n",
    "tiempo_nb_b = fin - inicio\n",
    "\n",
    "cm_nb_b = confusion_matrix(y_test_B, y_pred_nb_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d89aa1",
   "metadata": {},
   "source": [
    "### Naive Bayes Optimizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55074b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_opt = GaussianNB()\n",
    "\n",
    "param_grid = {\n",
    "    'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4]\n",
    "}\n",
    "\n",
    "# Crear Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=nb_opt,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='recall',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Ajustar modelo\n",
    "inicio = time.time()\n",
    "grid_search.fit(X_train_B_preprocessed, y_train_B)\n",
    "fin = time.time()\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Extraer mejor modelo\n",
    "best_model_nb = grid_search.best_estimator_\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred_nb_opt_b = best_model_nb.predict(X_test_B_preprocessed)\n",
    "\n",
    "tiempo_nb_opt_b = fin - inicio\n",
    "\n",
    "cm_nb_opt_b = confusion_matrix(y_test_B, y_pred_nb_opt_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194ab5c3",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145124a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inicio = time.time()\n",
    "svc.fit(X_train_B_preprocessed, y_train_B)\n",
    "fin = time.time()\n",
    "\n",
    "y_pred_svc_b = svc.predict(X_test_B_preprocessed)\n",
    "\n",
    "tiempo_svc_b = fin - inicio\n",
    "\n",
    "cm_svc = confusion_matrix(y_test_B, y_pred_svc_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdba2606",
   "metadata": {},
   "source": [
    "### Support Vector Machines Optimizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e5495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Crear Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=svc_opt,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='recall',\n",
    "    n_jobs=1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Ajustar modelo\n",
    "inicio = time.time()\n",
    "grid_search.fit(X_train_B_preprocessed, y_train_B)\n",
    "fin = time.time()\n",
    "\n",
    "# Extraer mejor modelo\n",
    "best_model_svc = grid_search.best_estimator_\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred_svc_opt_b = best_model_svc.predict(X_test_B_preprocessed)\n",
    "\n",
    "tiempo_svc_opt_b = fin - inicio\n",
    "\n",
    "cm_svc_opt_b = confusion_matrix(y_test_B, y_pred_svc_opt_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93b82b2",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69af2a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(random_state=42)\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "mlp.fit(X_train_A_preprocessed, y_train_A)\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred = mlp.predict(X_test_A_preprocessed)\n",
    "\n",
    "# Evaluar el rendimiento\n",
    "print(\"Matriz de confusi√≥n:\")\n",
    "print(confusion_matrix(y_test_A, y_pred))\n",
    "\n",
    "print(\"\\nReporte de clasificaci√≥n:\")\n",
    "print(classification_report(y_test_A, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ad4878",
   "metadata": {},
   "source": [
    "### MLP Optimizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6568a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "hidden_layer_options = [\n",
    "    (50,),\n",
    "    (100,),\n",
    "    (100, 50),\n",
    "    (100, 50, 25),\n",
    "    (150, 75),\n",
    "    (200, 100, 50)\n",
    "]\n",
    "\n",
    "activation_options = ['relu', 'tanh']\n",
    "solver_options = ['adam']\n",
    "learning_rates = [0.001, 0.0005, 0.0001]\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# B√öSQUEDA DE TODAS LAS COMBINACIONES\n",
    "# -------------------------------------------------------------------\n",
    "best_f1 = 0\n",
    "best_params = None\n",
    "\n",
    "total_tests = len(hidden_layer_options) * len(activation_options) * len(solver_options) * len(learning_rates)\n",
    "print(f\"\\nProbando {total_tests} combinaciones...\\n\")\n",
    "\n",
    "for hls, act, solv, lr in itertools.product(hidden_layer_options, activation_options, solver_options, learning_rates):\n",
    "\n",
    "    print(f\"Probando: hidden={hls}, activation={act}, solver={solv}, lr={lr}\")\n",
    "\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=hls,\n",
    "        activation=act,\n",
    "        solver=solv,\n",
    "        learning_rate_init=lr,\n",
    "        max_iter=500,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    mlp.fit(X_train_A_preprocessed, y_train_A)\n",
    "    y_pred = mlp.predict(X_test_A_preprocessed)\n",
    "    f1 = f1_score(y_test_A, y_pred, pos_label=1)  # F1 para la clase 1\n",
    "\n",
    "    print(f\" ‚Üí F1 (clase 1): {f1:.4f}\\n\")\n",
    "\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_params = (hls, act, solv, lr)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# RESULTADOS FINALES\n",
    "# -------------------------------------------------------------------\n",
    "print(\"========================================\")\n",
    "print(\"MEJOR COMBINACI√ìN ENCONTRADA:\")\n",
    "print(\"Hidden layers:\", best_params[0])\n",
    "print(\"Activation:\", best_params[1])\n",
    "print(\"Solver:\", best_params[2])\n",
    "print(\"Learning rate:\", best_params[3])\n",
    "print(f\"F1 (clase 1): {best_f1:.4f}\")\n",
    "print(\"========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a969982",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(150, 75),\n",
    "    activation='tanh',\n",
    "    solver='adam',\n",
    "    learning_rate_init=0.001,\n",
    "    max_iter=500,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "mlp.fit(X_train_A_preprocessed, y_train_A)\n",
    "\n",
    "y_pred_mlp_opt = mlp.predict(X_test_A_preprocessed)\n",
    "\n",
    "print(\"=== Classification Report ===\")\n",
    "print(classification_report(y_test_A, y_pred))\n",
    "\n",
    "# Matriz de confusi√≥n\n",
    "cm_mlp_opt = confusion_matrix(y_test_A, y_pred)\n",
    "\n",
    "sns.heatmap(cm_mlp_opt, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
    "plt.xlabel(\"Predicci√≥n\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.title(\"Matriz de confusi√≥n MLP\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90b98fa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîü **Evaluaci√≥n de modelos**\n",
    "Se analizan:\n",
    "- Matriz de confusi√≥n  \n",
    "- Classification report\n",
    "- Tiempo de predicci√≥n\n",
    "\n",
    "El objetivo es seleccionar el modelo m√°s robusto para la segmentaci√≥n de clientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ef1b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b81b7f57",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ **Variables m√°s importantes del mejor modelo**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40247735",
   "metadata": {},
   "source": [
    "## Regresi√≥n lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32296b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded2 = df.copy()\n",
    "\n",
    "df_encoded2 = df_encoded2.drop(columns=['pdays', 'contact', 'day', 'duration', 'campaign']) #, 'Patrimonio_Alto', 'Riesgo_Sobregiro'\n",
    "\n",
    "df_encoded2 = pd.get_dummies(df_encoded2, columns=['job', 'marital', 'education', 'default', 'housing', 'loan', 'month', 'poutcome', 'subscribed_term_deposit'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70ae4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_encoded2.drop(columns=['balance']) #'Patrimonio_Alto', 'Riesgo_Sobregiro', \n",
    "y = df_encoded2['balance']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb5d7ee",
   "metadata": {},
   "source": [
    "## Modelos lineales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b33139",
   "metadata": {},
   "source": [
    "## Regresi√≥n lineal simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624394dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar modelo de regresi√≥n lineal\n",
    "lr_model = LinearRegression()\n",
    "\n",
    "inicio = time.time()\n",
    "lr_model.fit(X_train, y_train)\n",
    "fin = time.time()\n",
    "\n",
    "lr_time = fin - inicio\n",
    "\n",
    "print(\"Tiempo LinearRegression:\")\n",
    "print(f\"{lr_time:.2f}\\n\")\n",
    "\n",
    "# Predicciones\n",
    "y_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluar modelo para train y para test\n",
    "print(\"Evaluaci√≥n del modelo de Regresi√≥n Lineal:\")\n",
    "lr_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "lr_r2 = r2_score(y_test, y_pred)\n",
    "print(f\"RMSE: {lr_rmse:.2f}\")\n",
    "print(f\"R¬≤: {lr_r2:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad255bd0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cce7f30",
   "metadata": {},
   "source": [
    "## Regularizaci√≥n Lasso\n",
    "\n",
    "El modelo Lasso aplica una penalizaci√≥n L1 sobre los coeficientes, lo que no solo ayuda a controlar la complejidad del modelo, sino que tambi√©n realiza selecci√≥n autom√°tica de variables, reduciendo a cero aquellas que aportan poca informaci√≥n.\n",
    "\n",
    "De este modo, permite simplificar el modelo sin perder interpretabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dce7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear y ajustar GridSearchCV para Lasso\n",
    "lasso = Lasso(max_iter=50000)\n",
    "\n",
    "alphas = np.logspace(-3, 1, 10)\n",
    "param_grid = {'alpha': alphas}\n",
    "\n",
    "grid_lasso = GridSearchCV(lasso, param_grid, cv=5, scoring='r2')\n",
    "\n",
    "inicio= time.time()\n",
    "grid_lasso.fit(X_train, y_train)\n",
    "fin = time.time()\n",
    "\n",
    "lasso_time = fin - inicio\n",
    "\n",
    "print(\"Tiempo Lasso:\")\n",
    "print(f\"{lasso_time:.2f}\")\n",
    "\n",
    "# Mejor modelo Lasso\n",
    "best_lasso = grid_lasso.best_estimator_\n",
    "print(f\"Mejor valor de alpha para Lasso: {grid_lasso.best_params_['alpha']:.6f}\")\n",
    "\n",
    "# Predicciones con el mejor modelo Lasso\n",
    "lasso_test_pred = best_lasso.predict(X_test)\n",
    "\n",
    "# Evaluar modelo\n",
    "print(\"\\nEvaluaci√≥n del modelo Lasso:\")\n",
    "lasso_rmse = np.sqrt(mean_squared_error(y_test, lasso_test_pred))\n",
    "lasso_r2 = r2_score(y_test, lasso_test_pred)\n",
    "\n",
    "print(f\"RMSE: {lasso_rmse:.2f}\")\n",
    "print(f\"R¬≤: {lasso_r2:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62503506",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "607d5cac",
   "metadata": {},
   "source": [
    "## Regularizaci√≥n Ridge\n",
    "\n",
    "Implementaremos Ridge con b√∫squeda de hiperpar√°metros usando GridSearchCV para encontrar el mejor valor de alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f687be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir par√°metros para b√∫squeda\n",
    "alphas = np.logspace(-3, 1, 10)\n",
    "param_grid = {'alpha': alphas}\n",
    "\n",
    "# Crear y ajustar GridSearchCV\n",
    "ridge = Ridge()\n",
    "grid_ridge = GridSearchCV(ridge, param_grid, cv=5, scoring='r2')\n",
    "\n",
    "inicio = time.time()\n",
    "grid_ridge.fit(X_train, y_train)\n",
    "fin = time.time()\n",
    "\n",
    "ridge_time = fin - inicio\n",
    "\n",
    "print(\"Tiempo Ridge:\")\n",
    "print(f\"{ridge_time:.2f}\")\n",
    "\n",
    "# Mejor modelo Ridge\n",
    "best_ridge = grid_ridge.best_estimator_\n",
    "print(f\"Mejor valor de alpha para Ridge: {grid_ridge.best_params_['alpha']:.6f}\")\n",
    "\n",
    "# Predicciones con el mejor modelo Ridge\n",
    "ridge_test_pred = best_ridge.predict(X_test)\n",
    "\n",
    "# Evaluar modelo\n",
    "print(\"\\nEvaluaci√≥n del modelo Ridge:\")\n",
    "ridge_rmse = np.sqrt(mean_squared_error(y_test, ridge_test_pred))\n",
    "ridge_r2 = r2_score(y_test, ridge_test_pred)\n",
    "\n",
    "print(f\"RMSE: {ridge_rmse:.2f}\")\n",
    "print(f\"R¬≤: {ridge_r2:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0387da",
   "metadata": {},
   "source": [
    "## An√°lisis de coeficientes e interpretaci√≥n\n",
    "\n",
    "Analizaremos los coeficientes del modelo Lasso para entender qu√© variables son las m√°s importantes para predecir el balance y cu√°les tienen mayor o menor impacto o son eliminadas por el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e2fc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener coeficientes del modelo Lasso\n",
    "feature_names = X_train.columns\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': best_lasso.coef_\n",
    "})\n",
    "\n",
    "# Ordenar coeficientes por valor absoluto\n",
    "coef_df['Abs_Coefficient'] = abs(coef_df['Coefficient'])\n",
    "coef_df_sorted = coef_df.sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "# Mostrar los 10 coeficientes m√°s importantes\n",
    "print(\"Top 10 variables m√°s importantes seg√∫n Lasso:\")\n",
    "print(coef_df_sorted.head(10))\n",
    "\n",
    "# Contar variables eliminadas (coeficientes = 0)\n",
    "zero_coef = len(coef_df[coef_df['Coefficient'] == 0])\n",
    "print(f\"\\nN√∫mero de variables eliminadas por Lasso (coeficiente = 0): {zero_coef}\")\n",
    "\n",
    "# Visualizar coeficientes m√°s importantes\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=coef_df_sorted.head(10), x='Coefficient', y='Feature')\n",
    "plt.title('Top 10 variables m√°s importantes (Lasso)')\n",
    "plt.xlabel('Coeficiente')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f766f3d1",
   "metadata": {},
   "source": [
    "## √Årboles y Bagging\n",
    "\n",
    "En esta secci√≥n nos enfocaremos en los modelos no lineales basados en √°rboles y su primer ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8376cf21",
   "metadata": {},
   "source": [
    "### An√°lisis de Overfitting en √Årboles de Regresi√≥n\n",
    "Este bloque de c√≥digo entrena m√∫ltiples modelos DecisionTreeRegressor variando la profundidad m√°xima del √°rbol (max_depth) para analizar c√≥mo afecta la complejidad del modelo al error de entrenamiento (train) y de prueba (test).\n",
    "\n",
    "Objetivo:\n",
    "- Evaluar el sobreajuste (overfitting) y subajuste (underfitting) observando la evoluci√≥n del RMSE en funci√≥n de la profundidad del √°rbol.\n",
    "- Determinar visualmente la profundidad √≥ptima que equilibra buen rendimiento en test sin sobreajustar los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977b5317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTreeRegressor\n",
    "max_depths = range(1, 18)\n",
    "train_rmses = []\n",
    "test_rmses = []\n",
    "\n",
    "for depth in max_depths:\n",
    "    tree = DecisionTreeRegressor(max_depth=depth, random_state=42)\n",
    "    tree.fit(X_train, y_train)\n",
    "    # Error en train\n",
    "    y_pred_train = tree.predict(X_train)\n",
    "    train_rmses.append(np.sqrt(mean_squared_error(y_train, y_pred_train)))\n",
    "    \n",
    "    # Error en test\n",
    "    y_pred_test = tree.predict(X_test)\n",
    "    test_rmses.append(np.sqrt(mean_squared_error(y_test, y_pred_test)))\n",
    "\n",
    "# Graficamos los resultados\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(max_depths, train_rmses, 'o-', label='Error de Entrenamiento (Train RMSE)')\n",
    "plt.plot(max_depths, test_rmses, 'o-', label='Error de Prueba (Test RMSE)')\n",
    "plt.xlabel('Profundidad del √Årbol (max_depth)')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Error del √Årbol vs. Profundidad', fontsize=16)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7557057",
   "metadata": {},
   "source": [
    "## Optimizaci√≥n de Decision Tree con GridSearchCV\n",
    "\n",
    "Este c√≥digo realiza una b√∫squeda de hiperpar√°metros para un DecisionTreeRegressor usando GridSearchCV. Se prueban distintos valores de max_depth, min_samples_split y min_samples_leaf para encontrar la combinaci√≥n que minimice el RMSE en validaci√≥n cruzada. Finalmente, se eval√∫a el modelo √≥ptimo en el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5824bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 6, 8],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=tree,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Entrenar la b√∫squeda\n",
    "inicio = time.time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "fin = time.time()\n",
    "\n",
    "tree_time = fin - inicio\n",
    "print(\"Tiempo Decision Tree (GridSearchCV):\")\n",
    "print(f\"{tree_time:.2f}\")\n",
    "\n",
    "# Calcular el RMSE\n",
    "mejor_modelo = grid_search.best_estimator_\n",
    "\n",
    "print(\"Mejores par√°metros del Decision Tree:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "y_pred = mejor_modelo.predict(X_test)\n",
    "\n",
    "# Evaluar modelo\n",
    "tree_r2_final = r2_score(y_test, y_pred)\n",
    "tree_rmse_final = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\"\\nEvaluaci√≥n del modelo Decision Tree (GridSearchCV):\")\n",
    "print(f\"RMSE: {tree_rmse_final:.2f}\")\n",
    "print(f\"R¬≤: {tree_r2_final:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24864e4c",
   "metadata": {},
   "source": [
    "## Optimizaci√≥n de Random Forest con GridSearchCV\n",
    "\n",
    "Este c√≥digo entrena un RandomForestRegressor buscando autom√°ticamente la mejor combinaci√≥n de hiperpar√°metros (n_estimators y max_depth) usando GridSearchCV. Se calcula el tiempo de entrenamiento y se eval√∫a el modelo final en el conjunto de prueba mediante RMSE y R¬≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124191b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestRegressor(random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 6, 8]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=forest,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,  # usa todos los n√∫cleos disponibles\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "inicio = time.time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "fin = time.time()\n",
    "forest_time = fin - inicio\n",
    "\n",
    "# Calcular el RMSE\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "forest_r2_final = r2_score(y_test, y_pred)\n",
    "forest_rmse_final = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "resultados_rf = pd.DataFrame({\n",
    "    \"Modelo\": [\"Random Forest (GridSearchCV)\"],\n",
    "    \"Tiempo Entrenamiento (s)\": [forest_time],\n",
    "    \"Mejores Par√°metros\": [grid_search.best_params_],\n",
    "    \"RMSE Test\": [forest_rmse_final],\n",
    "    \"R2 Test\": [forest_r2_final]\n",
    "})\n",
    "\n",
    "print(resultados_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bdee32",
   "metadata": {},
   "source": [
    "## Optimizaci√≥n de Gradient Boosting con GridSearchCV\n",
    "\n",
    "Este c√≥digo entrena un Gradient Boosting buscando autom√°ticamente la mejor combinaci√≥n de hiperpar√°metros (n_estimators, learning_rate y max_depth) usando GridSearchCV. Se calcula el tiempo de entrenamiento y se eval√∫a el modelo final en el conjunto de prueba mediante RMSE y R¬≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0e6628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Modelo Gradient Boosting (con GridSearchCV) ---\n",
    "print(\"Iniciando GridSearchCV para Gradient Boosting...\")\n",
    "print(\"Esto puede tardar varios minutos...\")\n",
    "\n",
    "# Definir el modelo\n",
    "gb_model = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Definir el grid de hiperpar√°metros\n",
    "# (Es un grid peque√±o para que no tarde una eternidad)\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [100, 150, 200],         # N√∫mero de √°rboles\n",
    "    'learning_rate': [0.03, 0.05, 0.1],       # Tasa de aprendizaje\n",
    "    'max_depth': [3, 6, 8]                 # Profundidad m√°xima\n",
    "}\n",
    "\n",
    "# Configurar GridSearchCV\n",
    "# Usamos scoring='neg_mean_squared_error' para optimizar RMSE\n",
    "# O 'r2' para ser consistentes con tu b√∫squeda anterior\n",
    "grid_gb = GridSearchCV(estimator=gb_model,\n",
    "                       param_grid=param_grid_gb,\n",
    "                       cv=5,  # 3-fold CV para ir m√°s r√°pido (puedes usar 5)\n",
    "                       scoring='r2',\n",
    "                       n_jobs=-1, # Usar todos los procesadores\n",
    "                       verbose=1)\n",
    "\n",
    "# Medir el tiempo de entrenamiento\n",
    "start_time = time.time()\n",
    "grid_gb.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "gb_time = end_time - start_time\n",
    "\n",
    "print(f\"\\nTiempo de entrenamiento Gradient Boosting (GridSearchCV): {gb_time:.2f} segundos\")\n",
    "print(f\"Mejores par√°metros encontrados: {grid_gb.best_params_}\")\n",
    "\n",
    "# Obtener el mejor modelo\n",
    "best_gb = grid_gb.best_estimator_\n",
    "\n",
    "# --- 2. Evaluaci√≥n del modelo Gradient Boosting ---\n",
    "\n",
    "print(\"\\nEvaluaci√≥n del modelo Gradient Boosting (con mejores hiperpar√°metros):\")\n",
    "\n",
    "# Predicciones\n",
    "gb_train_pred = best_gb.predict(X_train)\n",
    "gb_test_pred = best_gb.predict(X_test)\n",
    "\n",
    "# Evaluar\n",
    "gb_test_r2 = r2_score(y_test, gb_test_pred)\n",
    "gb_test_rmse = np.sqrt(mean_squared_error(y_test, gb_test_pred))\n",
    "\n",
    "print(f\"RMSE Test: {gb_test_rmse:.2f}\")\n",
    "print(f\"R¬≤ Test: {gb_test_r2:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7bfc07",
   "metadata": {},
   "source": [
    "## Tabla Maestra de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f4fedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Recolecci√≥n de M√©tricas ---\n",
    "# Asumimos que 'lr_model', 'best_lasso', y 'best_gb' S√ç existen en memoria de tu script anterior.\n",
    "\n",
    "# 1. Crear la lista de datos manualmente\n",
    "# Esta es la forma m√°s robusta de hacerlo, usando las variables de tu script.\n",
    "metric_data = [\n",
    "    # (Nombre, R¬≤, RMSE)\n",
    "    (\"Regresi√≥n Lineal\", lr_r2, lr_rmse, lr_time),\n",
    "    (\"Ridge\", ridge_r2, ridge_rmse, ridge_time),\n",
    "    (\"Lasso\", lasso_r2, lasso_rmse, lasso_time),\n",
    "    (\"Decission Tree Optimizado\", tree_r2_final, tree_rmse_final, tree_time),\n",
    "    (\"Random Forest Optimizado\", forest_r2_final, forest_rmse_final, forest_time),\n",
    "    (\"Gradient Boosting Optimizado\", gb_test_r2, gb_test_rmse, gb_time)\n",
    "]\n",
    "\n",
    "# 2. Construir el DataFrame\n",
    "resultados = []\n",
    "\n",
    "for nombre, r2, rmse, time in metric_data:\n",
    "    resultados.append({\n",
    "        \"Modelo\": nombre,\n",
    "        \"R¬≤ Test\": r2,\n",
    "        \"RMSE Test\": rmse,\n",
    "        \"Tiempo\": time\n",
    "    })\n",
    "\n",
    "# --- Crear la Tabla Maestra ---\n",
    "if resultados:\n",
    "    df_resultados = pd.DataFrame(resultados)\n",
    "    df_resultados = df_resultados.set_index('Modelo')\n",
    "    df_resultados = df_resultados.sort_values(by='R¬≤ Test', ascending=False)\n",
    "    \n",
    "    # Mostrar la tabla\n",
    "    print(\"\\n--- Tabla Maestra de Resultados (TEST set) ---\")\n",
    "    display(df_resultados.style.format({\n",
    "        'R¬≤ Test': '{:.4f}',\n",
    "        'RMSE Test': '{:,.2f}'\n",
    "    }))\n",
    "else:\n",
    "    print(\"La lista 'metric_data' est√° vac√≠a. Verifica las variables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4250ed3",
   "metadata": {},
   "source": [
    "Conclusiones de la tabla maestra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55b364b",
   "metadata": {},
   "source": [
    "## Comparar las Importancias de Variables\n",
    "\n",
    "Esta celda genera los tres gr√°ficos de importancia de variables uno al lado del otro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99bf70c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£2Ô∏è‚É£ **Conclusiones**\n",
    "- Principales hallazgos del EDA  \n",
    "- Interpretaci√≥n financiera de los resultados  \n",
    "- Mejor modelo encontrado y por qu√©  \n",
    "- Limitaciones del dataset  \n",
    "- Posibles mejoras o l√≠neas futuras  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84802433",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
